{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0554016c-17d3-434a-b259-2d5747c5baf2",
   "metadata": {},
   "source": [
    "# Estimating flood hazard from anonymized insurance data: A statistical simulation\n",
    "\n",
    "## Simulated data\n",
    "\n",
    "In this notebook, we use a simulation-based approach to evaluate statistical methods for estimating flood damage probabilities from anonymized insurance data. This allows us to compare the performance of different regression approaches in situations where the true values of estimated parameters are known—something that would not be possible under real-world conditions. In this simplified example, flood damage probabilities are modeled as a function of a single normally-distributed covariate, $X$, which is assumed to represent elevation: \n",
    "\n",
    "$$ X \\sim \\mathcal{N}(0,1) $$\n",
    "\n",
    "The values of $X$ are simulated across a 100 $\\times$ 100 square grid representing the study domain. In order to ensure that the study domain includes gradual transitions between areas of high and low elevation, spatial autocorrelation between values of $X$ is modeled using a spherical covariance function with a range of 50 gridcells. A total of 5000 gridcells are randomly selected as building points. At each building point, the probability of flood damage is computed as a logistic function of elevation: \n",
    "\n",
    "$$ p_i = \\frac{1}{1 + e^{-\\left(\\beta_0 + \\beta_1 x_i\\right)}} $$\n",
    "\n",
    "where $p_i$ represents the true flood hazard at building $i$, $x_i$ represents the building's elevation, and $\\beta_0$ and $\\beta_1$ are parameters controlling the relationship between elevation and flood hazard whose values are fixed at $-3$ and $-2$ respectively. In reality, $p_i$, $\\beta_0$, and $\\beta_1$ are never observed directly, and must be estimated based on past realizations of flood damage using regression analysis. Our objective is to identify regression approaches that yield accurate and unbiased estimates of these parameters. \n",
    "\n",
    "For each building point, the presence or absence of flood damage is simulated as a Bernoulli random variable whose expected value is equal to the true flood damage probabilty at that location:  \n",
    "\n",
    "$$ Y_i \\sim \\text{Bernoulli}(p_i) $$\n",
    "\n",
    "Similarly, the insurance status of each building is simulated as a Bernoulli random variable whose expected value is equal to 20%. Because we are unlikely to have access to information on the flood damage status of uninsured buildings, all regression models are fit using data from only insured buildings. \n",
    "\n",
    "To evaluate how imperfect information on the location of individual policyholders affects the performance of regression approaches, we divide our study domain into 25 equally-sized square blocks that are used to create an \"anonymized\" version of the insurance data. These polygons represent the coarse geographic identifiers that are typically reported by [OpenFEMA](https://www.fema.gov/openfema-data-page/fima-nfip-redacted-policies-v2) (e.g., census block groups, flood zones, etc.). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b91887c8-855e-433e-8d8a-12a1250d3095",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.linalg import cholesky\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.metrics as metrics\n",
    "from itertools import product\n",
    "from shapely.geometry import box\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7961cc1-3579-4bb8-a50e-3b3a8c09282f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### *** HELPER FUNCTIONS *** ###\n",
    "\n",
    "def create_regular_grid(x_points,y_points,crs='EPSG:4326'):\n",
    "    \"\"\"\n",
    "    param: x_points: numpy array of grid cell x coordinates (e.g., longitude). Assumes uniform spacing. \n",
    "    param: y_points: numpy array of grid cell y coordinates (e.g., latitude). Assumes uniform spacing. \n",
    "    param: crs: coordinate reference system of grid. \n",
    "    returns: grid_gdf: grid cell geodataframe. \n",
    "    \"\"\"\n",
    "    # Calculate grid cell spacing in x and y directions\n",
    "    width = np.diff(x_points)[0]\n",
    "    height = np.diff(y_points)[0]\n",
    "    \n",
    "    # Create list specifying coordinates of grid cell centers\n",
    "    coordinate_pairs = [(x,y) for x,y in product(x_points,y_points)]\n",
    "    x_vals = [x for x,y in coordinate_pairs]\n",
    "    y_vals = [y for x,y in coordinate_pairs]\n",
    "    grid_cells = [box(x-width/2,y-height/2,x+width/2,y+height/2) for x,y in coordinate_pairs]\n",
    "    grid_gdf = gpd.GeoDataFrame(data={'grid_x':x_vals,'grid_y':y_vals},geometry=grid_cells, crs=crs)\n",
    "    \n",
    "    return(grid_gdf)\n",
    "\n",
    "class UnivariateLogisticRegression:\n",
    "    \"\"\"\n",
    "    Univariate logistic regression class that allows user to manually adjust coefficients if desired\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,beta0,beta1):\n",
    "        self.beta0 = beta0\n",
    "        self.beta1 = beta1\n",
    "\n",
    "    def fit(self,x,y,sample_weight=None):\n",
    "\n",
    "        if sample_weight is None:\n",
    "            sample_weight = np.ones(len(x))\n",
    "        \n",
    "        mod = LogisticRegression()\n",
    "        mod.fit(x.reshape(len(x),1),y,sample_weight=sample_weight)\n",
    "        \n",
    "        self.beta0 = mod.intercept_[0]\n",
    "        self.beta1 = mod.coef_[0][0]\n",
    "\n",
    "    def predict_proba(self,x):\n",
    "        p = 1/(1+np.exp(-1*(self.beta0 + self.beta1*x)))\n",
    "        return p\n",
    "\n",
    "### *** FUNCTIONS AND CLASSES FOR SIMULATING SPATIAL RANDOM FIELDS *** ###\n",
    "\n",
    "class SphCovFun:\n",
    "    \"\"\"\n",
    "     Spherical covariance function class (child class of CovFun)\n",
    "    \"\"\"\n",
    "    def __init__(self,a):\n",
    "        \"\"\"\n",
    "        param: a: scale parameter determining speed at which spatial / temporal dependence decays\n",
    "        param: a_bounds: bounds on \"a\" if fitting as a free parameter \n",
    "        \"\"\"\n",
    "        self.a = a\n",
    "    \n",
    "    def cov(self,h):\n",
    "        \"\"\"\n",
    "        Return value of covariance function\n",
    "        param: h: value of spatial or temporal distance between points\n",
    "        \"\"\"\n",
    "        c = (1-np.heaviside(h - self.a,0))*(1 - 1.5*h/self.a + 0.5*h**3/self.a**3)\n",
    "        return c\n",
    "\n",
    "    def vgm(self,h):\n",
    "        \"\"\"\n",
    "        Return value of the variogram function\n",
    "        param: h: value of spatial or temporal distance between points\n",
    "        \"\"\"\n",
    "        v = self.cov(0) - self.cov(h)\n",
    "        return v\n",
    "    \n",
    "class ExpCovFun:\n",
    "    \"\"\"\n",
    "    Exponential covariance function class\n",
    "    \"\"\"\n",
    "    def __init__(self,a):\n",
    "        \"\"\"\n",
    "        param: a: scale parameter determining speed at which spatial / temporal dependence decays\n",
    "        param: a_bounds: bounds on \"a\" if fitting as a free parameter \n",
    "        \"\"\"\n",
    "        self.a = a\n",
    "    \n",
    "    def cov(self,h):\n",
    "        \"\"\"\n",
    "        Return value of covariance function\n",
    "        param: h: value of spatial or temporal distance between points\n",
    "        \"\"\"\n",
    "        c = np.exp(-h/self.a)\n",
    "        return c\n",
    "\n",
    "    def vgm(self,h):\n",
    "        \"\"\"\n",
    "        Return value of the variogram function\n",
    "        param: h: value of spatial or temporal distance between points\n",
    "        \"\"\"\n",
    "        v = self.cov(0) - self.cov(h)\n",
    "        return v\n",
    "\n",
    "def simulate_spatial_random_field(cov_fun,Nx,Ny,seed=None):\n",
    "    \"\"\"\n",
    "    This function simulates a spatially-correlated gaussian random variable.\n",
    "\n",
    "    param: cov_fun: fully parameterized covariance function / variogram model (instance of class CovFun)\n",
    "    param: Nx: Number of gridcells in x-direction\n",
    "    param: Ny: Number of gridcells in y-direction\n",
    "    param: seed: optional fixed random seed for reproducibility\n",
    "    returns: XX: x-coordinate grid (Nx x Ny array)\n",
    "    returns: YY: y-coordinate grid (Nx x Ny array)\n",
    "    returns: ZZ: simulated values of random variable (Nx x Ny array)\n",
    "    \"\"\"\n",
    "\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed=seed)\n",
    "\n",
    "    XX,YY = np.meshgrid(np.arange(0,Nx,1),np.arange(0,Ny,1))\n",
    "    coords = np.vstack([XX.ravel(), YY.ravel()]).T\n",
    "    \n",
    "    dmat = squareform(pdist(coords))\n",
    "    cov_mat = cov_fun.cov(dmat)\n",
    "    \n",
    "    L = cholesky(cov_mat, lower=True, check_finite=True)\n",
    "    u = stats.norm.rvs(size=len(coords))\n",
    "    \n",
    "    z = L @ u\n",
    "    ZZ = z.reshape(XX.shape)\n",
    "\n",
    "    return XX, YY, ZZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7930685f-44b1-4b63-b9fc-26824f296cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "### *** GENERATE SYNTHETIC DATA *** ###\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "seed = 112263\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Simulate a spatial random field representing an enironmental predictor.\n",
    "# For simplicitly, we'll refer to this as \"elevation\", but could be anything. \n",
    "Nx=100\n",
    "Ny=100\n",
    "cov_fun = SphCovFun(a=50)\n",
    "XX, YY, ZZ = simulate_spatial_random_field(cov_fun,Nx,Ny)\n",
    "\n",
    "N_coords = len(XX.ravel())\n",
    "coord_indices = np.arange(N_coords)\n",
    "\n",
    "# Randomly place buildings across study domain and sample elevations\n",
    "# Save as pandas geodataframe\n",
    "N_buildings = 5000\n",
    "building_indices = np.random.choice(coord_indices,size=N_buildings,replace=False)\n",
    "building_X = XX.ravel()[building_indices]\n",
    "building_Y = YY.ravel()[building_indices]\n",
    "building_Z = ZZ.ravel()[building_indices]\n",
    "buildings = gpd.GeoDataFrame(data={'elevation':building_Z},geometry=gpd.points_from_xy(building_X,building_Y))\n",
    "buildings.index.name = 'building_id'\n",
    "buildings.reset_index(inplace=True)\n",
    "\n",
    "# Specify flood hazard function, and draw for whether each building was flooded\n",
    "beta0 = -3  # Intercept in logistic model\n",
    "beta1 = -2  # Coefficient in logistic model\n",
    "flood_hazard = UnivariateLogisticRegression(beta0,beta1)\n",
    "buildings['flood_hazard'] = flood_hazard.predict_proba(buildings['elevation'])\n",
    "buildings['flooded'] = stats.bernoulli.rvs(p=buildings['flood_hazard'].to_numpy(),size=len(buildings))\n",
    "\n",
    "# Draw for whether each building is insured or uninsured\n",
    "insured_prob = 0.2\n",
    "buildings['insured'] = stats.bernoulli.rvs(p=insured_prob,size=len(buildings))\n",
    "\n",
    "# Create polygons that we'll use for aggregation\n",
    "# (these will be square tiles, but could represent something like census blocks) \n",
    "width = 20\n",
    "height = 20\n",
    "polygons = create_regular_grid(np.arange(0+width/2,Nx,width),np.arange(0+height/2,Ny,height),crs=None)\n",
    "polygons['polygon_id'] = polygons.index.values\n",
    "\n",
    "# Join polygon info to buildings\n",
    "buildings = gpd.sjoin(buildings,polygons[['polygon_id','geometry']],how='left').drop(columns='index_right')\n",
    "\n",
    "# Sometimes a building will fall on the border of two polygons, resulting in duplicate rows\n",
    "# Keep only the first polygon that a building joins to\n",
    "buildings.drop_duplicates(subset='building_id',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d696e36b-f027-4536-8a97-3f5199ecb33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### *** CREATE PLOT OF STUDY DOMAIN *** ###\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "\n",
    "ax.pcolormesh(XX,YY,ZZ,cmap='terrain',vmin=-3,vmax=3)\n",
    "\n",
    "polygons.plot(ax=ax,facecolor='none',edgecolor='k')\n",
    "\n",
    "m1 = (buildings['insured']==1)&(buildings['flooded']==1)\n",
    "m2 = (buildings['insured']==1)&(buildings['flooded']==0)\n",
    "buildings[m2].plot(ax=ax,marker='o',facecolor='none',edgecolor='k')\n",
    "buildings[m1].plot(ax=ax,marker='x',color='r',linewidth=2)\n",
    "\n",
    "ax.set_xlim([0,Nx])\n",
    "ax.set_ylim([0,Ny])\n",
    "\n",
    "ax.set_xlabel('x-coordinate')\n",
    "ax.set_ylabel('y-coordinate')\n",
    "ax.set_title('Study domain',fontweight='bold')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9f2909-2407-4a9b-b95d-7e5b11f47ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### *** CREATE PLOT OF FLOOD HAZARD FUNCTION *** ###\n",
    "\n",
    "fig,ax1 = plt.subplots(figsize=(6,4))\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "ax1.hist(buildings['elevation'],bins=30,alpha=0.5)\n",
    "ax1.set_xlabel('Elevation')\n",
    "ax1.set_ylabel('Frequency',color='C0')\n",
    "ax1.tick_params(axis='y', colors='C0')\n",
    "\n",
    "xmin = -3.5\n",
    "xmax = 3.5\n",
    "xvals = np.linspace(xmin,xmax,200)\n",
    "yvals = flood_hazard.predict_proba(xvals)\n",
    "\n",
    "ax2.plot(xvals,yvals,color='C3',lw=2)\n",
    "ax2.set_xlim([xmin,xmax])\n",
    "ax2.set_ylim([0,1])\n",
    "ax2.set_ylabel('Flood damage probability',color='C3')\n",
    "ax2.tick_params(axis='y', colors='C3')\n",
    "\n",
    "ax1.set_title('Modeled Flood Hazard vs. Elevation',fontweight='bold')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd0d945-abf8-46bf-9950-9ca32b3cd718",
   "metadata": {},
   "outputs": [],
   "source": [
    "### *** DISPLAY STRUCTURE OF SIMULATED DATA *** ###\n",
    "buildings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae43b3d-1d40-440e-846c-6e4541ef216f",
   "metadata": {},
   "source": [
    "## Performance metrics\n",
    "\n",
    "To evaluate the performance of different regression approaches, we will compute the mean squared error (MSE) between the predicted flood damage probability ($\\hat{p_i}$) and true flood damage probability ($p_i$) of each building: \n",
    "\n",
    "$$ MSE = \\frac{1}{n} \\sum_{i=1}^n \\left( p_i - \\hat{p_i}\\right)^2 $$\n",
    "\n",
    "Because we are dealing with probabilities that are typically small, it is helpful to anchor the MSE of each regression model against that of a naive reference model. This is done by computing a skill score (SS) that reflects the degree to which a skilled model reduces MSE compared to a naive model:\n",
    "\n",
    "$$ SS = 1 - \\frac{MSE_{model}}{MSE_{ref}} $$\n",
    "\n",
    "where $MSE_{ref}$ represents the MSE of a naive model that predicts all buildings to have a flood damage probability equal to the sample mean (i.e., an intercept-only model). \n",
    "\n",
    "$$ MSE_{ref} = \\frac{1}{n} \\sum_{i=1}^n \\left( p_i - \\bar{p}\\right)^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a900d4ca-6f5a-49d6-b2dd-328b905aa84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_metrics(p_true,p_pred,p_bar=None):\n",
    "    \"\"\"\n",
    "    param: p_true: true flood damage probability\n",
    "    param: p_pred: predicted flood damage probability\n",
    "    param: p_bar: naive prediction of flood damage probability (e.g., share of buildings flooded)\n",
    "    \"\"\"\n",
    "\n",
    "    if p_bar is None:\n",
    "        p_bar = np.mean(p_true)\n",
    "\n",
    "    MSE = np.mean((p_true-p_pred)**2)\n",
    "    MSE_ref = np.mean((p_true-p_bar)**2)\n",
    "\n",
    "    performance = {}\n",
    "    performance['MSE'] = MSE\n",
    "    performance['SS'] = 1 - MSE/MSE_ref\n",
    "    \n",
    "    return(performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7654cc07-b8d8-4d2f-b29c-c36b4703778c",
   "metadata": {},
   "source": [
    "## Model A: Address-level data\n",
    "\n",
    "First, we evaluate the predictive performance of a logistic regression model fit to address-level insurance data: \n",
    "\n",
    "$$ \\log \\left( \\frac{p_i}{1 - p_i} \\right) = \\beta_0 + \\beta_1 x_i$$\n",
    "\n",
    "Because we simulated the flood hazard of each building as a logistic function of elevation, we know that the above regression model is correctly specified. However, the estimated values of $\\beta_0$ and $\\beta_1$ may differ slightly from their true values due to the limited sample size of insured buildings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e617a50-8266-43d5-aca8-c68111ac9602",
   "metadata": {},
   "outputs": [],
   "source": [
    "### *** MODEL A: ADDRESS-LEVEL DATA ***  ###\n",
    "\n",
    "# Get observations from insured buildings (dropping uninsured)\n",
    "address_level_insurance_data = buildings[buildings['insured']==1].reset_index(drop=True)\n",
    "address_level_insurance_data.index.name = 'record_id'\n",
    "address_level_insurance_data.reset_index(inplace=True)\n",
    "address_level_insurance_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a104f67-d7d4-4007-bd72-26b36585f150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt to fit a logistic regression model\n",
    "modA = UnivariateLogisticRegression(0,0)\n",
    "x = address_level_insurance_data['elevation'].to_numpy()\n",
    "y = address_level_insurance_data['flooded'].to_numpy()\n",
    "modA.fit(x,y)\n",
    "\n",
    "# Compute performance metrics\n",
    "p_pred = modA.predict_proba(buildings['elevation'].to_numpy())\n",
    "p_true = buildings['flood_hazard'].to_numpy()\n",
    "p_bar = np.mean(y)\n",
    "perf_modA = performance_metrics(p_true,p_pred,p_bar=p_bar)\n",
    "\n",
    "# Print results\n",
    "print('\\n*** Model A: Address-level data ***\\n')\n",
    "print(f'beta0 = {modA.beta0:.3f}')\n",
    "print(f'beta1 = {modA.beta1:.3f}')\n",
    "print(f'MSE = {perf_modA['MSE']:.6f}')\n",
    "print(f'SS  = {perf_modA['SS']:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370e1d55-a115-4f13-9683-9b555566b139",
   "metadata": {},
   "source": [
    "## Model B: Anonymized data with stochastic assignment of buildings\n",
    "\n",
    "To evaluate how imperfect information on the location of individual policyholders affects the performance of regression approaches, we will attempt to fit a logistic regression model to an \"anonymized\" version of the insurance data where building-specific information has been removed. This means that we do not know which building generated a given presence or absence point; however, we can narrow down the list of potential building candidates based on the `polygon_id` associated with each insurance record. Within each polygon, we randomly assign presence-absence points to buildings in order to assign building-specific covariate values such as elevation. This process assumes all buildings within a polygon are equally likely to be selected and is repeated 1,000 times to ensure that the space of possible assignments is adequately explored.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67d057a-57ae-42e1-a989-bd1edb4d4d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "### *** MODEL B: ANONYMIZED DATA WITH STOCHASTIC ASSIGNMENT OF BUILDINGS *** ###\n",
    "\n",
    "# Drop information about building location and elevation.\n",
    "# Now we only know the flood damage status of each record and the polygon where it is located. \n",
    "anonymized_insurance_data = address_level_insurance_data[['record_id','flooded','polygon_id']]\n",
    "anonymized_insurance_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41d217d-d0e7-4fb7-80b2-af5317548aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get counts of presence / absence points within each polygon\n",
    "polygon_pa_counts = anonymized_insurance_data.groupby('polygon_id').agg({'flooded':['count','sum']})\n",
    "polygon_pa_counts.columns = ['num_records','num_presence']\n",
    "polygon_pa_counts['num_absence'] = polygon_pa_counts['num_records']-polygon_pa_counts['num_presence']\n",
    "polygon_pa_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57665583-8f2b-4fc2-9be9-7922197425f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lookup table that returns buildings associated with each polygon\n",
    "building_lookup = buildings[['polygon_id','building_id','elevation']].sort_values(by='polygon_id').set_index('polygon_id')\n",
    "building_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8778c49-4a3a-470e-9e65-2bc119565e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create functions that we can use to randomly assign presence-absence points to buildings\n",
    "# within each polygon\n",
    "\n",
    "def assign_presence_absence_points(polygon_id,num_presence,num_absence,building_lookup):\n",
    "    \"\"\"\n",
    "    param: polygon_id: unique id of polygon of interest\n",
    "    param: num_presence: number of presence points to sample within polygon\n",
    "    param: num_absence: number of absence points to sample within polygon\n",
    "    param: building_lookup: pandas dataframe listing buildings associated with each polygon, indexed by polygon_id\n",
    "    \"\"\"\n",
    "\n",
    "    sampled_buildings = building_lookup.loc[polygon_id].sample(num_presence+num_absence)\n",
    "    flooded_status = np.zeros(len(sampled_buildings),dtype=int)\n",
    "    flooded_inds = np.random.choice(np.arange(len(sampled_buildings)),size=num_presence,replace=False)\n",
    "    flooded_status[flooded_inds] = 1\n",
    "    sampled_buildings['flooded'] = flooded_status\n",
    "    \n",
    "    return(sampled_buildings)\n",
    "\n",
    "def monte_carlo_sample_buildings(polygon_pa_counts,building_lookup,num_replicates=1):\n",
    "    \"\"\"\n",
    "    param: polygon_pa_counts: pandas dataframe listing number of presence-absence points in each polygon, indexed by polygon_id\n",
    "    param: building_lookup: pandas dataframe listing buildings associated with each polygon, indexed by polygon_id\n",
    "    param: num_replicates: number of times to repeat monte carlo sampling\n",
    "    \"\"\"\n",
    "    \n",
    "    df_list = []\n",
    "    \n",
    "    for i in range(num_replicates):\n",
    "        \n",
    "        df = pd.concat([assign_presence_absence_points(polygon_id,row['num_presence'],row['num_absence'],building_lookup) for polygon_id,row in polygon_pa_counts.iterrows()])\n",
    "        df.reset_index(inplace=True)\n",
    "        df['replicate'] = i+1\n",
    "        df_list.append(df)\n",
    "\n",
    "    df = pd.concat(df_list).reset_index(drop=True).rename(columns={'building_id':'candidate_building_id'})\n",
    "\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53b398d-7f46-4aea-8a7e-233f6a007e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stochastically assign presence-absence points to buildings. \n",
    "# Do this 1000 times to ensure we adequately sample potential options. \n",
    "mc_data = monte_carlo_sample_buildings(polygon_pa_counts,building_lookup,num_replicates=1000)\n",
    "mc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f592c9c1-ac4e-40b0-9d6e-e27468d8f774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt to fit a logistic regression model\n",
    "modB = UnivariateLogisticRegression(0,0)\n",
    "x = mc_data['elevation'].to_numpy()\n",
    "y = mc_data['flooded'].to_numpy()\n",
    "modB.fit(x,y)\n",
    "\n",
    "# Compute performance metrics\n",
    "p_pred = modB.predict_proba(buildings['elevation'].to_numpy())\n",
    "perf_modB = performance_metrics(p_true,p_pred,p_bar=p_bar)\n",
    "\n",
    "# Print results\n",
    "print('\\n*** Model B: Anonymized data with stochastic assignment of buildings ***\\n')\n",
    "print(f'beta0 = {modB.beta0:.3f}')\n",
    "print(f'beta1 = {modB.beta1:.3f}')\n",
    "print(f'MSE = {perf_modB['MSE']:.6f}')\n",
    "print(f'SS  = {perf_modB['SS']:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35a0add-cfaf-4475-b2b5-e2a0e10edef5",
   "metadata": {},
   "source": [
    "## Model C: Weighted regression with uniform weights\n",
    "\n",
    "Next, we explore an alternative method for dealing with anonymized insurance data that formulates the problem as a weighted regression. To do this, we start by defining the set of $K_i$ building candidates that could potentially be associated with insurance record $i$ based on its `polygon_id`:\n",
    "\n",
    "$$ C_i = \\left\\{b_{i1}, b_{i2}, b_{i3}, ..., b_{iK_i}\\right\\}$$\n",
    "\n",
    "where $b_{ij}$ represents the $j$th building candidate for insurance record $i$. Next, we define a latent categorical variable $Z_i$ that represents the specific building in the candidate set that generated insurance record $i$: \n",
    "\n",
    "$$ Z_i \\in C_i $$\n",
    "\n",
    "Because the insurance data is anonymized, we cannot observe the value of $Z_i$ directly. However, if we assume that all building candidates are equally likely, we can define the \"weight\" associated with potential values of $Z_i$ as follows:\n",
    "\n",
    "$$ w_{ij} = \\text{Prob}(Z_i = b_{ij}) = \\frac{1}{K_i} $$\n",
    "\n",
    "This implies that an insurance record with 10 building candidates will give a weight of 0.1 to each potential record-building match. This approach weights each potential match equally, even if some potential matches are unrealistic (e.g., an insurance record that flooded is unlikely to have been generated by a building with a very high elevation). \n",
    "\n",
    "The above weights are used within a weighted logistic regression that estimates $\\beta_0$ and $\\beta_1$ by maximizing the following log-likelihood function: \n",
    "\n",
    "$$ \\ell \\left(\\beta_0, \\beta_1\\right) = \\sum_{i=1}^{n} \\sum_{j=1}^{K_i} w_{ij} \\left[ y_i \\log p_{ij} + \\left(1-y_i\\right)\\log\\left(1-p_{ij}\\right)\\right]$$\n",
    "\n",
    "where $y_i$ represents the observed outcome of insurance record $i$ (i.e., flooded or not flooded), and $p_{ij}$ represents the probability that building candidate $j$ flooded based on its elevation ($x_{ij}$) and current estimates of $\\beta_0$ and $\\beta_1$: \n",
    "\n",
    "$$ p_{ij} = \\frac{1}{1 + e^{-\\left(\\beta_0 + \\beta_1 x_{ij}\\right)}} $$\n",
    "\n",
    "Although all of this mathematical notation might seem like overkill for what is a fairly simple regression problem, it is helpful to introduce these concepts so that we can later derive improved methods for weighting building candidates.\n",
    "\n",
    "The Monte Carlo approach employed by model B should be asymptotically equivalent to the weighted regression shown above when the number of replicates is large. As such, we can expect models B and C to return similar estimates of $\\beta_0$ and $\\beta_1$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e53998-65af-44bf-b7eb-e9d2698ec544",
   "metadata": {},
   "outputs": [],
   "source": [
    "### *** MODEL C: WEIGHTED REGRSSION WITH UNIFORM WEIGHTS *** ###\n",
    "\n",
    "# Create dataframe where each row represents a potential match between a \n",
    "# specific insurance record and building located within the same polygon\n",
    "candidate_df = pd.merge(anonymized_insurance_data,building_lookup,on='polygon_id',how='left').rename(columns={'building_id':'candidate_building_id'})\n",
    "\n",
    "# Assume that for a given record, all buildings within its polygon are equally likely\n",
    "candidate_df['candidate_weight'] = 1\n",
    "\n",
    "# Display structure of data\n",
    "candidate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7875408-ce5a-4100-87a8-e737853c5cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that will allow us to normalize the weight assigned to each record-building \n",
    "# match so that the weights associated with each insurance record sum to 1.0 \n",
    "\n",
    "def normalize_weights(df,id_col='record_id',weight_col='candidate_weight'):\n",
    "    \"\"\"\n",
    "    This function normalizes the weights assigned to potential building matches so that \n",
    "    the weights associated with each insurance record sum to 1.0. \n",
    "    \n",
    "    param: df: dataframe where each row represents a potential record-building match\n",
    "    param: id_col: name of column containing unique ID of each record\n",
    "    param: weight_col: name of column containing weight assigned to each building\n",
    "    \"\"\"\n",
    "    \n",
    "    df[weight_col] = df.groupby(id_col)[weight_col].transform(lambda x: x / x.sum())\n",
    "    return(df)\n",
    "\n",
    "# Apply function to dataframe\n",
    "candidate_df = normalize_weights(candidate_df)\n",
    "candidate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67049a61-6f07-4ea0-b6c4-22a83aa3fa32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt to fit a logistic regression model\n",
    "modC = UnivariateLogisticRegression(0,0)\n",
    "x = candidate_df['elevation'].to_numpy()\n",
    "y = candidate_df['flooded'].to_numpy()\n",
    "w = candidate_df['candidate_weight'].to_numpy()\n",
    "modC.fit(x,y,sample_weight=w)\n",
    "\n",
    "# Compute performance metrics\n",
    "p_pred = modC.predict_proba(buildings['elevation'].to_numpy())\n",
    "perf_modC = performance_metrics(p_true,p_pred,p_bar=p_bar)\n",
    "\n",
    "# Print results\n",
    "print('\\n*** Model C: Weighted regression with uniform weights ***\\n')\n",
    "print(f'beta0 = {modC.beta0:.3f}')\n",
    "print(f'beta1 = {modC.beta1:.3f}')\n",
    "print(f'MSE = {perf_modC['MSE']:.6f}')\n",
    "print(f'SS  = {perf_modC['SS']:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321e99a1-2f93-4c43-a4e3-ccf033678100",
   "metadata": {},
   "source": [
    "## Model D: Iterative reweighting via the EM algorithm\n",
    "\n",
    "The weighting scheme used by model C treats all building candidates as equally likely. In reality, we know that this is not the case—for example, insurance claims are more likely to be generated by buildings with high flood hazard than those with low flood hazard. Ideally, we would like to assign more weight to building candidates with high flood hazard for insurance records that resulted in a claim (presence points). Similarly, we would like to assign more weight to building candidates with low flood hazard for insurance records that did not result in a claim (absence points). However, doing this accurately requires prior knowledge of the flood hazard at each building—the quantity we are trying to estimate—which creates a catch-22 scenario. \n",
    "\n",
    "To overcome this catch-22 scenario, we can employ iterative methods that use an initial guess of building candidate weights to produce an approximate estimate of the flood hazard at each building, which can then be used in the next iteration to produce a more accurate estimate of building candidate weights and flood hazard. By repeating this process a number of times, we can generate increasingly accurate estimates of flood hazard. \n",
    "\n",
    "An iterative method that is well-suited to our regression problem is the [expectation-maximization (EM) algorithm](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm). This algorithm is designed to find maximum likelihood (MLE) or maximum a posteriori (MAP) estimates of model parameters for statistical models that depend on unobserved latent variables—in our case, the building from which an insurance record was generated ($Z_i$). In each EM iteration, building candidate weights are updated via the application of Bayes rule: \n",
    "\n",
    "\n",
    "$$ w_{ij}^{new} = \\underbrace{\\text{Prob}(Z_i = b_{ij} \\mid y_i )}_{\\text{Posterior}} \\propto \\underbrace{\\text{Prob}(y_i \\mid Z_i = b_{ij})}_{\\text{Likelihood}} \\times \\underbrace{\\text{Prob}(Z_i = b_{ij})}_{\\text{Prior}} $$\n",
    "\n",
    "In the above equation, $\\text{Prob}(Z_i = b_{ij})$ represents our prior belief that a building candidate generated an insurance record *before* observing any data. For simplicity, we select a uniform (non-informative) prior that assumes all building candidates are equally likely: \n",
    "\n",
    "$$ \\text{Prob}(Z_i = b_{ij}) = \\frac{1}{K_i} $$\n",
    "\n",
    "This is equivalent to the \"uniform weights\" used within model C. The use of uniform priors ensures that the EM algorithm will return MLE estimates of model parameters; if we were to specify a non-uniform prior (i.e., if we believe some buildings are more likely than others) then we would instead be performing a MAP estimation. Because our priors are uniform and fixed, our updated weight estimates will be driven by the likelihood function, which reflects the probability that a given building candidate was flooded based on current estimates of model parameters: \n",
    "\n",
    "$$ \\text{Prob}(y_i \\mid Z_i = b_{ij}) = p_{ij}^{y_i}\\left(1 - p_{ij}\\right)^{1 - y_i} \\approx \\hat{p}_{ij}^{y_i}\\left(1 - \\hat{p}_{ij}\\right)^{1 - y_i}$$\n",
    "\n",
    "where $y_i$ represents the observed outcome for insurance record $i$ (1 = flooded, 0 = not flooded), and $\\hat{p}_{ij}$ represents the estimated probability that building candidate $b_{ij}$ was flooded based on the current iteration's estimates of $\\beta_0$ and $\\beta_1$. The right hand side of the above equation is the probability mass function (PMF) for a Bernoulli random variable (our assumed distribution for $Y_i$). \n",
    "\n",
    "The pseudo-code of the EM algorithm is shown below: \n",
    "\n",
    "1. Initialization\n",
    "    * Provide an initial guess for building candidate weights based on prior beliefs (for uniform prior, set $w_{ij} = 1 / K_i$).\n",
    "    * Estimate $\\beta_0$ and $\\beta_1$ based on initial guess using weighted logistic regression.\n",
    "2. E-step: Update building candidate weights\n",
    "    * Calculate likelihood of each building candidate based on current values of $\\beta_0$ and $\\beta_1$.\n",
    "    * Calculate updated value of building candidate weights using Bayes rule ($w_{ij}^{new} =$ Likelihood $\\times$ Prior).\n",
    "    * Normalize weights for each insurance record so that $\\sum_j w_{ij}^{new} = 1.0$.\n",
    "3. M-step: Update model coefficients\n",
    "    * Fit a weighted logistic regression using updated building candidate weights.\n",
    "    * Record updated values of $\\beta_0$ and $\\beta_1$.\n",
    "4. Test for convergence\n",
    "    * Check whether $\\text{max}\\left(|w^{old} - w^{new}|\\right)$ is less than the desired error tolerance.\n",
    "    * If not, repeat steps 2-3 until convergence is reached. \n",
    "    * If so, stop and return the final estimates of $\\beta_0$ and $\\beta_1$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f104d68a-2e4c-40c5-a0c0-9b623392d776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EM_algorithm(df,mod,x_col,y_col,weight_col,id_col,tol=1e-6,maxiter=100,verbose=False):\n",
    "    \"\"\"\n",
    "    This function estimates building candidate weights and model coefficients\n",
    "    using the expectation-maximization (EM) algorithm. \n",
    "\n",
    "    param: df: \n",
    "    \"\"\"\n",
    "\n",
    "    x = df[x_col].to_numpy()\n",
    "    y = df[y_col].to_numpy()\n",
    "\n",
    "    ## Initialization\n",
    "\n",
    "    # Specify a uniform, fixed prior for weights\n",
    "    df[weight_col] = 1\n",
    "    df = normalize_weights(df,id_col=id_col,weight_col=weight_col)\n",
    "    prior = df[weight_col].to_numpy()\n",
    "\n",
    "    # Fit model using uniform weights\n",
    "    mod.fit(x,y,sample_weight=prior)\n",
    "\n",
    "    # Print initial values of model parameters\n",
    "    if verbose: \n",
    "        print('\\n*** EXPECTATION-MAXIMIZATION ALGORITHM ***\\n',flush=True)\n",
    "        print(f'(0) beta0={mod.beta0:.3f}  beta1={mod.beta1:.3f}',flush=True)\n",
    "\n",
    "    keepgoing = True\n",
    "    converged = False\n",
    "    numiter = 1\n",
    "\n",
    "    while keepgoing and (numiter <= maxiter): \n",
    "\n",
    "        ## E-step: Update sample weights\n",
    "    \n",
    "        # Record current values of building candidate weights.  \n",
    "        # (will eventually use to assess convergence)\n",
    "        w_old = df[weight_col].to_numpy()\n",
    "    \n",
    "        # Compute likelihood of observed outcomes at each building candidate\n",
    "        # based on current values of model parameters. \n",
    "        p = mod.predict_proba(x)\n",
    "        likelihood = stats.bernoulli.pmf(y,p)\n",
    "    \n",
    "        # Bayesian update of weights\n",
    "        df[weight_col] = likelihood*prior\n",
    "        df = normalize_weights(df,id_col=id_col,weight_col=weight_col)\n",
    "        w_new = df[weight_col].to_numpy()\n",
    "    \n",
    "        ## M-step: Update model parameters\n",
    "    \n",
    "        # Fit the model again using new weights\n",
    "        mod.fit(x,y,sample_weight=w_new)\n",
    "    \n",
    "        ## Check for convergence\n",
    "        abs_change = np.abs(w_new-w_old)\n",
    "    \n",
    "        if max(abs_change) < tol:\n",
    "            keepgoing = False\n",
    "            converged = True\n",
    "\n",
    "        # Print update\n",
    "        if verbose: \n",
    "            print(f'({numiter}) beta0={mod.beta0:.3f}  beta1={mod.beta1:.3f}  max(abs(w_new-w_old))={max(abs_change):.6f}',flush=True)\n",
    "\n",
    "        # Increment counter\n",
    "        numiter += 1\n",
    "\n",
    "    if verbose:\n",
    "        if converged: \n",
    "            print(f'\\n*** Converged (tol={tol}) ***\\n',flush=True)\n",
    "        else:\n",
    "            print(f'\\n*** Failed to converge (maxiter={maxiter}) ***\\n',flush=True)\n",
    "            \n",
    "    return(mod,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042ad1c9-9219-4d00-9bfb-ff570f5b1631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model parameters and weights using EM algorithm\n",
    "modD = UnivariateLogisticRegression(0,0)\n",
    "modD,candidate_df = EM_algorithm(candidate_df,modD,'elevation','flooded','candidate_weight','record_id',verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e681c9b-e88e-4435-bd2a-0f138348d2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute performance metrics\n",
    "p_pred = modD.predict_proba(buildings['elevation'].to_numpy())\n",
    "perf_modD = performance_metrics(p_true,p_pred,p_bar=p_bar)\n",
    "\n",
    "# Print results\n",
    "print('\\n*** Model D: Iterative reweighting via the EM algorithm ***\\n')\n",
    "print(f'beta0 = {modD.beta0:.3f}')\n",
    "print(f'beta1 = {modD.beta1:.3f}')\n",
    "print(f'MSE = {perf_modD['MSE']:.6f}')\n",
    "print(f'SS  = {perf_modD['SS']:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa9d5f4-04da-423c-94b1-20a21ccd61af",
   "metadata": {},
   "source": [
    "# Performance comparison\n",
    "\n",
    "In order to compare the relative performance of models A-D, we fit the models to 1000 simulated datasets generated using different random seeds. This allows us to evaluate the sampling distribution of $\\hat{\\beta_0}$ and $\\hat{\\beta_1}$ under different regression approaches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d81ecae-84f4-4bb6-8912-2df56e36ba1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8227770-e4e5-4a1a-8908-f32049bfab29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fli-env-v1",
   "language": "python",
   "name": "fli-env-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
